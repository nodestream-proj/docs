"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[3522],{4282:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"/2025/02/24/nodestream-0-14","metadata":{"permalink":"/docs/blog/2025/02/24/nodestream-0-14","editUrl":"https://github.com/nodesteram-proj/docs/tree/main/packages/create-docusaurus/templates/shared/blog/2025-02-24-nodestream-0-14/index.md","source":"@site/blog/2025-02-24-nodestream-0-14/index.md","title":"Nodestream 0.14 Release","description":"We are happy to announce the release of Nodestream 0.14.","date":"2025-02-24T00:00:00.000Z","formattedDate":"February 24, 2025","tags":[{"label":"release","permalink":"/docs/blog/tags/release"},{"label":"nodestream","permalink":"/docs/blog/tags/nodestream"}],"readingTime":2.46,"hasTruncateMarker":false,"authors":[{"name":"Zach Probst","title":"Maintainer of Nodestream","url":"https://github.com/zprobst","imageURL":"https://github.com/zprobst.png","key":"zprobst"}],"frontMatter":{"title":"Nodestream 0.14 Release","authors":["zprobst"],"tags":["release","nodestream"]},"unlisted":false,"nextItem":{"title":"Nodestream 0.13 Release","permalink":"/docs/blog/2024/08/09/nodestream-0-13"}},"content":"We are happy to announce the release of Nodestream 0.14.\\nThis release includes a number of new features and improvements. \\nHowever, its breaking changes are minimal, so upgrading should be a breeze. \\n\\n## Breaking Changes\\n\\n### File Extractors \\n\\nIn `0.13`, we introduced a united file handling extractor. However, we kept the old extractors around for backwards compatibility. \\nStarting with this release, we have removed the old extractors and everything is now handled by \\nthe `UnifiedFileExtractor` extractor which is now renamed to `FileExtractor`.\\n\\n**Check out the docs on it [here](/docs/docs/reference/extractors/#the-file-extractor)**.\\n\\nCore Contributors to this feature include:\\n- [Zach Probst](https://github.com/zprobst)\\n- [Angelo Santos](https://github.com/angelosantos4)\\n\\n## New Features\\n\\n## Object Storage APIs \\n\\nNodestream now has a new object storage abstraction. \\nThese APIs allow steps in your pipeline to interact with object storage to persist data between executions.\\nWe see incredible value in this feature as it allows for more complex pipelines to be built. \\nWe\'ve implemented serveral features in this relase that leverage this new abstraction. \\n\\nYou can persist objects locally or in the cloud via AWS S3. \\nLike large amounts of the framework it is plubbable and can be extended to support other object storage providers.\\n\\n**Check out the docs on it [here](/docs/docs/reference/object-storage/)**.\\n\\nCore Contributors to this feature include:\\n- [Zach Probst](https://github.com/zprobst)\\n- [Angelo Santos](https://github.com/angelosantos4)\\n- [Chad Cloes](https://github.com/ccloes)\\n\\n## Extractor Checkpointing\\n\\nPreviously, extractors would always start from the beginning of their data source. \\nIf a pipeline crashed or was interrupted, the extractor would start from the beginning of the data source again. \\nThis lead to duplicate data being extracted, processed, and inserted into the database. \\n\\nNow with nodestream 0.14, extractors can now checkpoint their progress. \\nThis means that if a pipeline crashes or is interrupted, the extractor will be start from where it left off. \\nTo do this, the extractor will store a checkpoint via its object storage.\\nTherefore, in order to use this feature, you must have object storage configured during your pipeline execution.\\nCheckpoints are cleared when a pipeline is successfully completed.\\n\\nCurious how to implement this for your extractor? We\'ve update the tutorial on it [here](/docs/docs/tutorials-advanced/new-steps/#creating-an-extractor).\\n\\n\\nCore Contributors to this feature include:\\n- [Zach Probst](https://github.com/zprobst)\\n- [Angelo Santos](https://github.com/angelosantos4)\\n- [Chad Cloes](https://github.com/ccloes)\\n\\n## Record Schema Enforcement\\n\\nNodestream now has a new record schema enforcement feature.\\nNodestream pipelines tend to be highly dependent on the schema of the data being processed. \\nDepending on the data source, the schema can change over time. \\nThis can lead to pipelines failing or producing incorrect results. \\n\\nWith this new feature, you can now enforce a schema on your data.\\nThis means that if the schema of the data changes, the pipeline will skip or warn about\\nthe records that do not match the schema. \\n\\nNot only that, but you can also use this feature to automatically infer the \\nschema of your data and then enforce it. \\n\\n**Check out the docs on it [here](/docs/docs/reference/filters/)**.\\n\\nCore Contributors to this feature include:\\n- [Zach Probst](https://github.com/zprobst)\\n- [Angelo Santos](https://github.com/angelosantos4)\\n- [Chad Cloes](https://github.com/ccloes)"},{"id":"/2024/08/09/nodestream-0-13","metadata":{"permalink":"/docs/blog/2024/08/09/nodestream-0-13","editUrl":"https://github.com/nodesteram-proj/docs/tree/main/packages/create-docusaurus/templates/shared/blog/2024-08-09-nodestream-0-13/index.md","source":"@site/blog/2024-08-09-nodestream-0-13/index.md","title":"Nodestream 0.13 Release","description":"We are happy to announce the release of Nodestream 0.13.","date":"2024-08-09T00:00:00.000Z","formattedDate":"August 9, 2024","tags":[{"label":"release","permalink":"/docs/blog/tags/release"},{"label":"nodestream","permalink":"/docs/blog/tags/nodestream"}],"readingTime":4.7,"hasTruncateMarker":false,"authors":[{"name":"Zach Probst","title":"Maintainer of Nodestream","url":"https://github.com/zprobst","imageURL":"https://github.com/zprobst.png","key":"zprobst"}],"frontMatter":{"title":"Nodestream 0.13 Release","authors":["zprobst"],"tags":["release","nodestream"]},"unlisted":false,"prevItem":{"title":"Nodestream 0.14 Release","permalink":"/docs/blog/2025/02/24/nodestream-0-14"},"nextItem":{"title":"Migrations Design in Nodestream 0.12","permalink":"/docs/blog/2024/05/14/migrations-evolution"}},"content":"We are happy to announce the release of Nodestream 0.13.\\nThis release includes a number of new features and improvements. \\nHowever, its breaking changes are minimal, so upgrading should be a breeze. \\n\\n## Breaking Changes\\n\\n### Unified File Extractors \\n\\nIn the past, we had separate file extractors for local, remote, and S3 files. \\nThis was a bit cumbersome for a couple of reasons: \\n\\n- On the maintainability side, we had to make sure that all of these extractors were kept in sync with each other. \\n- On the user side, it was limiting to have to choose between these extractors when the only difference was the location of the file.\\n\\nStarting with this release, we have unified these extractors into a single `UnifiedFileExtractor` extractor. \\nThis extractor can handle local, remote, and S3 files (so functionality is not lost). \\n\\n**Check out the docs on it [here](/docs/docs/reference/extractors/#the-file-extractor)**.\\n\\nCore Contributors to this feature include:\\n- [Zach Probst](https://github.com/zprobst)\\n- [Angelo Santos](https://github.com/angelosantos4)\\n\\n### Changing Source Node Behavior With `properties`\\n\\nIn the past, the `properties` key automatically lower cased all string property values. \\nThis was because there was one set of normalization rules for the entire source node interpretation.\\nHowever, this was a bit limiting because it was not possible to have different normalization rules for different properties.\\n\\nStarting with this release, the `properties` key no longer automatically lower cases all string property values.\\nInstead, you can now define normalization rules for keys and properties separately (via the `key_normalization` and `property_normalization` properties).\\nHowever, if you specify the `normalization` key, it will apply to both keys and properties and will default to lower casing all string property values. \\n\\n## New Features\\n\\n### Squashing Migrations\\n\\nIn the past, migrations were applied one by one in order. \\nIf during development, you were constantly iterating on a data model you could be constantly adding migrations. \\nThis resulted in a lot of migrations being applied that were essentially intermediary when going to production.\\nAs a result, the migration node count could get quite large with a lot of \\"messy\\" migrations.\\n\\nStarting with this release, you can now squash migrations.\\nThis means that you can take a set of migrations and squash them into a single, optimized set of migrations.\\nThis can be useful for cleaning up the migration node count and making it easier to understand the data model. \\nAdditionally, the old migrations are still stored in the project, so you can always go back to them if you need to. \\nIf a database has partially applied a sequence of migrations that was squashed, we can\'t used the squashed migration.\\nInstead, the logic will fall back to the original migrations. \\n\\nCore Contributors to this feature include:\\n- [Zach Probst](https://github.com/zprobst)\\n- [Angelo Santos](https://github.com/angelosantos4)\\n\\n**Check out the docs on it [here](/docs/docs/tutorials-intermediate/working-with-migrations#squash-migrations)**.\\n\\n### Compressed File Handling\\n\\nMany users have data stored in compressed files.\\nThis release adds support for compressed files that are `.gz`, `.bz2` in format.\\nThis support is available in the `UnifiedFileExtractor` extractor.\\n\\n**Check out the docs on it [here](/docs/docs/reference/extractors#the-file-extractor)**.\\n\\nCore Contributors to this feature include:\\n- [Grant Hoffman](https://github.com/grantleehoffman)\\n- [Zach Probst](https://github.com/zprobst)\\n\\n### Improved LLM Compatible Schema Printing \\n\\nThe `llm` format is a format that is used to represent the schema of a graph. \\nThis release adds improved support for printing the schema in a format that is compatible with an llm.\\n\\nIn short, it uses a cypher-esque syntax to represent the schema of the graph:\\n\\n```cypher\\nNode Types:\\nPerson: first_name: STRING, last_name: STRING, last_ingested_at: DATETIME, age: STRING\\nNumber: number: STRING, last_ingested_at: DATETIME\\nRelationship Types:\\nKNOWS: last_ingested_at: DATETIME\\nAdjancies:\\n(:Person)-[:KNOWS]->(:Person)\\n```\\n\\nCore Contributors to this feature include:\\n- [Zach Probst](https://github.com/zprobst)\\n- [Angelo Santos](https://github.com/angelosantos4)\\n\\n### Improved Error Messages in Value Providers \\n\\nNodestream uses value providers to extract values from documents and map them to graph.\\nEvery time you get an error in a value provider, it can be a bit tricky to debug. \\nThis release adds improved error messages to value providers to make it easier to debug issues.\\n\\nCore Contributors to this feature include:\\n- [Yason Khaburzaniya](https://github.com/yasonk)\\n- [Zach Probst](https://github.com/zprobst)\\n\\n### DynamoDB Extractor\\n\\nDynamoDB is a popular NoSQL database that is used by many people to store data.\\nThis release adds support for DynamoDB as a first class citizen via the `DynamoDBExtractor`.\\n\\n**Check out the docs on it [here](/docs/docs/reference/extractors#dynamodbextractor)**.\\n\\nCore Contributors to this feature include:\\n- [Angelo Santos](https://github.com/angelosantos4)\\n- [Zach Probst](https://github.com/zprobst)\\n\\n### SQS and Queue Extractor Support \\n\\nMany users have data stored in SQS and other queue services.\\nThis release adds support for SQS and other queue services via the `QueueExtractor`.\\nConcecptually, this extractor is similar to the `StreamExtractor` but for queues.\\n\\n**Check out the docs on it [here](/docs/docs/reference/extractors#queueconnector)**.\\n\\nCore Contributors to this feature include:\\n- [Grant Hoffman](https://github.com/grantleehoffman)\\n- [Zach Probst](https://github.com/zprobst)\\n\\n### Release Attestations \\n\\n`0.13` marks the first release were nodestream and all of its dependencies are signed and attested to \\nvia [Github\'s Attestation support](https://github.blog/news-insights/product-news/introducing-artifact-attestations-now-in-public-beta/). This means that you can be sure that the code you are running is the code that was intended to be run.\\n\\n### Dependency Updates \\n\\nThis release includes updates to dependencies to keep Nodestream up to date with the latest and greatest. \\nSome dependencies that were updated include:\\n\\n- `httpx` to `>=0.27.0`\\n- `uvloop` to `>=0.17.0, <=0.19.0` (Not installed/used on Python 3.13 due to compatibility issues)\\n- `numpy` to `>=2.0.0`\\n- `pyarrow` to `17.0.0`\\n- `python` 3.13 has been added to the supported versions matrix.\\n- A variety of other dependencies have had their supported versions widened to be more permissive.\\n\\n### Bug Fixes\\n\\n- Fixed a bug where schema inference was not working correctly in some cases (with switch interpretations).\\n- Fixed a variety of bugs related to the pipeline runtime that were causing mishandled errors."},{"id":"/2024/05/14/migrations-evolution","metadata":{"permalink":"/docs/blog/2024/05/14/migrations-evolution","editUrl":"https://github.com/nodesteram-proj/docs/tree/main/packages/create-docusaurus/templates/shared/blog/2024-05-14-migrations-evolution/index.md","source":"@site/blog/2024-05-14-migrations-evolution/index.md","title":"Migrations Design in Nodestream 0.12","description":"In the release notes for Nodestream 0.12, we mentioned that we had added support for migrations.","date":"2024-05-14T00:00:00.000Z","formattedDate":"May 14, 2024","tags":[{"label":"migrations","permalink":"/docs/blog/tags/migrations"},{"label":"nodestream","permalink":"/docs/blog/tags/nodestream"}],"readingTime":6.82,"hasTruncateMarker":false,"authors":[{"name":"Zach Probst","title":"Maintainer of Nodestream","url":"https://github.com/zprobst","imageURL":"https://github.com/zprobst.png","key":"zprobst"}],"frontMatter":{"title":"Migrations Design in Nodestream 0.12","authors":["zprobst"],"tags":["migrations","nodestream"]},"unlisted":false,"prevItem":{"title":"Nodestream 0.13 Release","permalink":"/docs/blog/2024/08/09/nodestream-0-13"},"nextItem":{"title":"Nodestream Neptune Support","permalink":"/docs/blog/2024/04/26/nodestream-neptune-support"}},"content":"In the release notes for Nodestream 0.12, we mentioned that we had added support for migrations. \\nThis is a feature that we have been wanting to add for a long time, and we are excited to finally have it in place. \\nIn this post, we will discuss what migrations are, why they are important, and how they work in Nodestream.\\n\\n## Evolutionary Database Design\\n\\nEvolutionary database design is the idea that the database schema should evolve over time as the application changes.\\nThis is in contrast to the traditional approach of creating a fixed schema at the beginning of a project and then never changing it.\\nWith evolutionary database design, the schema is treated as a living document that can be updated and modified as needed.\\nIf you want to go deep into this topic, we recommend reading the [Martin Fowler\'s page](https://martinfowler.com/articles/evodb.html) on the subject.\\n\\n## Why Migrations?\\n\\nMigrations are a way to manage the evolution of the database schema in a controlled and repeatable way.\\nThey allow you to define the changes that need to be made to the schema in a series of files that can be run in sequence.\\nThis makes it easy to track changes to the schema over time and to apply those changes to multiple environments, such as development, staging, and production.\\n\\n## Surveying All The Types of Schema Changes\\n\\nGraph databases are schema-less, but the data model is still defined by the relationships between nodes and edges and the properties of those nodes and edges.\\nThis means that there is still a schema to manage, even if it is not as rigid as a traditional relational database schema.\\nSince nodestream is agnostic to the underlying database, we need to be able to support migrations for all types of databases that nodestream can work with.\\nTherefore we need to support migrations that are designed against an abstract graph model and leave the implementation details to the specific database connector.\\nSo lets examine the types of schema changes that can exist in a graph database:\\n\\n### Creating New Nodes and Edges Types \\n\\nThe most basic type of schema change is creating new node and edge types.\\nThis is equivalent to creating a new table in a relational database.\\nWhen you create a new node or edge type, you may need to define the properties that it will have and the relationships that it will have with other nodes and edges.\\n\\nDepending on the underlying database, this might involve creating a new index or constraint to enforce the uniqueness of the new node or edge type.\\n\\n### Removing Nodes and Edges Types\\n\\nConversely, you may also need to remove existing node and edge types.\\nThis is equivalent to dropping a table in a relational database.\\nMost graph databases do not support leaving orphaned nodes or edges, so you may need to delete all nodes and edges of the type that you are removing. \\n\\n### Adding Properties to Nodes and Edges\\n\\nAnother common type of schema change is adding properties to existing nodes and edges.\\nThis is equivalent to adding a new column to a table in a relational database.\\nWhen you add a property to a node or edge, you may need to define a default value for that property or update existing nodes and edges to have a value for that property.\\n\\nOne tricky case is when you add a property that is part of the nodes or edges key.\\nIn this case, you may need to update the key of the node or edge to include the new property.\\n\\n### Removing Properties from Nodes and Edges\\n\\nConversely, you may also need to remove properties from existing nodes and edges.\\nThis is equivalent to dropping a column from a table in a relational database.\\nWhen you remove a property from a node or edge, you may need to update existing nodes and edges to remove the value for that property.\\n\\n### Adding and Removing Indexes\\n\\nAnother common type of schema change is adding and removing indexes.\\nIndexes are used to speed up queries by allowing the database to quickly find nodes and edges that match certain criteria.\\nWhen you add an index, you may need to define the properties that the index will be based on and the type of index that will be used.\\nWhen you remove an index, you may need to update existing indexes to remove the properties that the index was based on.\\n\\n### Topological Changes\\n\\nFinally, you may need to make topological changes to the schema such as adding or removing relationships between nodes and edges.\\nThis is equivalent to adding or removing foreign keys in a relational database.\\n\\nWhen you change the adjancency of nodes and edges, you may want to clean up the data to ensure that it is consistent with the new schema.\\nThis may involve updating existing nodes and edges to reflect the new relationships or deleting nodes and edges that are no longer needed.\\n\\n## How Migrations Work in Nodestream\\n\\nIn nodestream, migrations are defined as a series of yaml files that describe the changes that need to be made to the schema.\\nEach migration file contains a list of operations that need to be performed. \\nFor example, creating a new node type or adding a property to an existing node type.\\n\\nWhen you run `nodestream migrations make` nodestream will create a new migration file in the `migrations` directory. \\nThat process works roughly like this:\\n\\n- Nodestream will look at the current state of the schema by initializing and introspecting all pipelines (A).\\n- Build the state of the schema that is represented by the current migrations (B).\\n- Diff the two states (A and B) to determine the changes that need to be made to the schema.\\n- Generate a new migration file that describes the changes that need to be made to the schema.\\n\\nWhen you run `nodestream migrations run` nodestream will apply the migrations in sequence to evolve the schema.\\nThat process works roughly like this:\\n\\n- Nodestream reads the migration files into memory and builds a graph of the dependencies between the migrations.\\n- Nodestream runs the migrations in topological order, applying the changes to the schema as it goes.\\n- Nodestream keeps track of which migrations have been applied so that it can skip them in the future.\\n\\nCrucially, nodestream does not track all possible schema changes. \\nTopological changes are not tracked(see [here](#topological-changes)), so you will need to handle those manually.\\nAdditionally, nodestream does not support rolling back migrations, so you will need to handle that manually as well.\\n\\n## Are Migrations Any Good?\\n\\nWondering what Martin Fowler would think of this design given is [page on the subject](https://martinfowler.com/articles/evodb.html)? \\nHe describes the concept of \\"evolutionary database design\\" with a set of characterisitcs. \\nSome of them are more organizational than technical.\\n\\nHowever, some of the technical ones are:\\n\\n- **All Database Artifacts are Version Controlled with Application Code:** Nodestream\'s migrations are intended to be source controlled files that are run in sequence and define their dependencies. This makes it easy to evolve changes and continuously integrate them (which is another of the characteristics).\\n- **All database changes are database refactorings** Nodestream\'s migrations are a series of database refactorings that are run in sequence. This makes it easy to track changes to the schema over time and to apply those changes to multiple environments, such as development, staging, and production. We are detecting the changes that need to be made to the schema and applying them in a controlled and repeatable way.\\n- **Clearly Separate Database Access Code** You generally don\'t need to write database access code in nodestream, so this is taken care of :fireworks:\\n- **Automated the Refactorings** This is the main point of migrations. They are automated and can be run in sequence to evolve the schema.\\n\\nWe are happy with the design of the migrations in nodestream and we think that they are a good fit for the project. \\nAs we\'ve mentioned, there are still some major evolutions to be made to migrations, such as the ability to rollback a migration, but we are confident that we are on the right track."},{"id":"/2024/04/26/nodestream-neptune-support","metadata":{"permalink":"/docs/blog/2024/04/26/nodestream-neptune-support","editUrl":"https://github.com/nodesteram-proj/docs/tree/main/packages/create-docusaurus/templates/shared/blog/2024-04-26-nodestream-neptune-support/index.md","source":"@site/blog/2024-04-26-nodestream-neptune-support/index.md","title":"Nodestream Neptune Support","description":"The recent release of Nodestream 0.12 has introduced support for Amazon Neptune as the first step towards broader multi-database support. Nodestream provides a flexible tool to perform bulk ETL into Amazon Neptune Database and Amazon Neptune Analytics.","date":"2024-04-26T00:00:00.000Z","formattedDate":"April 26, 2024","tags":[{"label":"neptune","permalink":"/docs/blog/tags/neptune"},{"label":"nodestream","permalink":"/docs/blog/tags/nodestream"}],"readingTime":2.81,"hasTruncateMarker":false,"authors":[{"name":"Cole Greer","title":"Neptune Plugin Maintainer","url":"https://github.com/Cole-Greer","imageURL":"https://github.com/Cole-Greer.png","key":"Cole-Greer"}],"frontMatter":{"title":"Nodestream Neptune Support","authors":["Cole-Greer"],"tags":["neptune","nodestream"]},"unlisted":false,"prevItem":{"title":"Migrations Design in Nodestream 0.12","permalink":"/docs/blog/2024/05/14/migrations-evolution"},"nextItem":{"title":"Nodestream 0.12 Release","permalink":"/docs/blog/2024/04/05/nodestream-0-12"}},"content":"The recent [release of Nodestream 0.12](/docs/blog/2024/04/05/nodestream-0-12/) has introduced support for Amazon Neptune as the first step towards broader multi-database support. Nodestream provides a flexible tool to perform bulk ETL into Amazon Neptune Database and Amazon Neptune Analytics.\\n\\nThis post will give a quick overview of the new Amazon Neptune support, offer some examples to get started, and list some features planned for future releases.\\n\\n## Overview\\n\\nSupport for AWS Neptune is split into two modes, DB and Analytics. Both modes leverage the AWS SDK to load data via batched openCypher queries. Nodestream is compatible with Neptune DB engine version 1.2.1.1 or higher, as well as Neptune Analytics.\\n\\n## Capabilities\\n\\nNodestream with Neptune currently supports standard ETL pipelines as well as time to live (TTL) pipelines. ETL pipelines enable bulk data ingestion into Neptune from a much broader range of data sources and formats than have previously been possible in Neptune.\\n\\n[Nodestream\'s TTL mechanism](/docs/docs/tutorials-intermediate/removing-data/) also enables new capabilities not previously available in Neptune. By annotating ingested graph elements with timestamps, Nodestream is able to create pipelines which automatically expire and remove data that has passed a configured lifespan.\\n\\n## Usage\\n\\n### Prerequisites\\n\\nNeptune must be reachable from whichever environment you intend to run Nodestream. Both Neptune Database, as well as Neptune Analytics with a private endpoint, are restricted to VPC-only access. If you intend to use a Neptune Analytics graph with a public endpoint, no special considerations are required.\\n\\nCheck out the [Neptune User-Guide](https://docs.aws.amazon.com/neptune/latest/userguide/get-started-connecting.html) for more information about connecting to a VPC-only host. You can test the connection with this curl command:\\n\\n```shell\\ncurl https://<NEPTUNE_ENDPOINT>:<PORT>/openCypher/status\\n```\\n\\n### IAM Auth\\n\\nNodestream fully supports IAM Authentication when connecting to Amazon Neptune, as long as credentials are properly configured. See the [boto3 credentials guide](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html#configuring-credentials) for more instruction on correctly configuring credentials.\\n\\n### Configuration Examples\\n\\nThe connection configuration for Neptune contains a switch between two modes: `db` and `analytics`. Neptune DB mode will connect using a Neptune Database cluster or instance endpoint, while Neptune Analytics will connect via the graph identifier.\\n\\n#### Neptune Database:\\n\\n```yaml\\ntargets:\\n  db-one:\\n    database: neptune\\n    mode: database\\n    host: https://<NEPTUNE_ENDPOINT>:<PORT>\\n```\\n\\n#### Neptune Analytics:\\n\\n```yaml\\ntargets:\\n  db-one:\\n    database: neptune\\n    mode: analytics\\n    graph_id: <GRAPH_IDENTIFIER>\\n```\\n\\nCheck out the [Nodestream basics tutorial](/docs/docs/category/tutorial---basics/) for a complete guide to getting started with Nodestream and Neptune.\\n\\n## Future Roadmap\\n\\nWe have several new features planned to bring additional functionality in upcoming releases.\\n\\nOne feature we are excited to bring to the Nodestream Neptune plugin is support for the new Nodestream Migrations API. Some migrations are not applicable in Neptune as it does not use user-defined indices. However, support for applicable migrations, such as renaming properties, will be added in an upcoming release.\\n\\nWe are additionally planning to add expanded datatype support. Currently, the Neptune plugin supports string, boolean, and numeric types. Datetime types are automatically converted into epoch timestamps. We aim to expand this list such that any extracted types which are supported by Neptune can be loaded without casting or conversion.\\n\\nOur future work will also include further performance assessments and optimizations. We will continue to optimize the generated queries in order to maximize the performance and scalability of Nodestream with Neptune.\\n\\n## Get Involved\\n\\nThe inclusion of new features is heavily dependent on community feedback, if there are any additional features or configurations which you would find valuable, please create an issue on [GitHub](https://github.com/nodestream-proj/nodestream-plugin-neptune/issues) with the request."},{"id":"/2024/04/05/nodestream-0-12","metadata":{"permalink":"/docs/blog/2024/04/05/nodestream-0-12","editUrl":"https://github.com/nodesteram-proj/docs/tree/main/packages/create-docusaurus/templates/shared/blog/2024-04-05-nodestream-0-12/index.md","source":"@site/blog/2024-04-05-nodestream-0-12/index.md","title":"Nodestream 0.12 Release","description":"We are happy to announce the release of Nodestream 0.12.","date":"2024-04-05T00:00:00.000Z","formattedDate":"April 5, 2024","tags":[{"label":"release","permalink":"/docs/blog/tags/release"},{"label":"nodestream","permalink":"/docs/blog/tags/nodestream"}],"readingTime":4.005,"hasTruncateMarker":false,"authors":[{"name":"Zach Probst","title":"Maintainer of Nodestream","url":"https://github.com/zprobst","imageURL":"https://github.com/zprobst.png","key":"zprobst"}],"frontMatter":{"title":"Nodestream 0.12 Release","authors":["zprobst"],"tags":["release","nodestream"]},"unlisted":false,"prevItem":{"title":"Nodestream Neptune Support","permalink":"/docs/blog/2024/04/26/nodestream-neptune-support"},"nextItem":{"title":"Software Vulnerability Analysis using SBOMs, Amazon Neptune, and Nodestream","permalink":"/docs/blog/2024/04/05/nodestream-sbom-preview"}},"content":"We are happy to announce the release of Nodestream 0.12. \\nThis release marks the largest update to Nodestream since its inception. \\nWe\'ve spent a lot of time improving the core of nodestream and we\'re excited to share it with you.\\n\\nBefore we get into the details, we want to thank the community for their support and feedback. \\nAs such, we have completely revamped the documentation to make it easier to use and navigate.\\nThis releases comes with two headline features [Database Migrations](#database-migrations) and [Multi-Database Support](#multi-database-support).\\n\\n## Major Features\\n\\n### Database Migrations \\n\\nIn the past, nodestream attempted to automatically create indexes and constraints on the database based on your pipeline at runtime.\\nThis was done by introspecting the schema of the entire project and generating the appropriate queries to create the indexes and constraints.\\nThis was a very powerful feature but it had a few drawbacks:\\n- **It was redundant.** The same indexes and constraints were being created with `IF NOT EXISTS` clauses every time the pipeline was run.\\n- **It was slow.** The queries were being executed serially and the pipeline was locked until they were all completed.\\n- **It was error prone.** If the database was not in a state that allowed for the creation of the indexes and constraints, the pipeline would fail.\\n- **It was high friction.** There was no way to refactor the database without manual intervention. If the schema changed, the pipeline would fail and the user would have to manually remove the indexes, constraints, and sometimes data before running the pipeline again.\\n\\nTo address these issues, `nodestream` 0.12 has introduced the concept of migrations.\\nMigrations are a way of encapsulating changes to the database schema in a way that can be applied incrementally. \\nConceptually, they are similar to the migrations in the [Django](https://docs.djangoproject.com/en/5.0/topics/migrations/), [Rails](https://guides.rubyonrails.org/v3.2/migrations.html), [Neo4j Migrations](https://neo4j.com/labs/neo4j-migrations/2.0/), and [Flyway](https://documentation.red-gate.com/fd/migrations-184127470.html) frameworks.\\n\\n![Database Migrations](./migrations.gif)\\n\\nMigrations are defined in a directory called `migrations` in the root of your project.\\nEach migration is a yaml file that contains data about the migration and its dependencies.\\nYou can create migrations by running the `nodestream migrations make` command.\\n\\nCheck out the changes to the tutorial on [Database Migrations](/docs/docs/tutorial-basics/prepare-your-database) as well as the new tutorial on [Working With Migrations](/docs/docs/tutorials-intermediate/working-with-migrations) to learn more.\\n\\nCore Contributors to this feature include:\\n- [Zach Probst](https://github.com/zprobst)\\n- [Grant Hoffman](https://github.com/grantleehoffman)\\n- [Yason Khaburzaniya](https://github.com/yasonk)\\n- [Chad Cloes](https://github.com/ccloes)\\n- [Angelo Santos](https://github.com/angelosantos4)\\n\\n### Multi-Database Support \\n\\nPrior to this release, the only database that was supported was neo4j. \\nWhile this is a category leading database, the goal of nodestream is to be database agnostic and afford developer the ability to use the database or _databases_ that best fits their needs. \\nAs such, we are happy to announce that nodestream now supports [Amazon Neptune](https://aws.amazon.com/neptune/) and [Amazon Neptune Analytics](https://docs.aws.amazon.com/neptune-analytics/latest/userguide/what-is-neptune-analytics.html).\\nTO accommodate that, we have moved the neo4j database connector into a separate package called [nodestream-plugin-neo4j](https://pypi.org/project/nodestream-plugin-neo4j/) and added a new package called [nodestream-plugin-neptune](https://pypi.org/project/nodestream-plugin-neptune/).\\n\\nStarting with this release, you use the `--database` flag to generate neptune boilerplate configuration. \\n\\n![Database Migrations](./neptune.gif)\\n\\n**Check out the docs on it [here](/docs/docs/databases/neptune/)**.\\n\\nCore Contributors to this feature include:\\n- [Zach Probst](https://github.com/zprobst)\\n- [Alex Le](https://github.com/aryex)\\n- [Cole Greer](https://github.com/Cole-Greer)\\n- [Dave Bechberger](https://github.com/bechbd)\\n- [Alexey Temnikov](https://github.com/alexey-temnikov)\\n- [Yang Xia](https://github.com/xiazcy)\\n\\n## Other Features\\n\\n### Parquet Support\\n\\nMany customers have data stored in parquet format. \\nParquet is a columnar storage format that is optimized for reading and writing large datasets.\\nWe are happy to announce that nodestream now supports parquet as a first class citizen.\\n\\n**Check out the docs on it [here](/docs/docs/reference/extractors/#the-file-extractor)**.\\n\\nCore Contributors to this feature include:\\n- [Zach Probst](https://github.com/zprobst)\\n- [Dave Bechberger](https://github.com/bechbd)\\n- [Cole Greer](https://github.com/Cole-Greer)\\n- [Leszek Kurzyna](https://github.com/leszek-bq)\\n\\n### Include Properties From Maps\\n\\nIn the past, each property you wanted to include in the pipeline had to be explicitly defined in the pipeline configuration. \\nThis was a bit cumbersome and error prone.\\nStarting with this release, you can now include all properties by defining an expression that returns a map at the `properties` key directly instead of a mapping of property names to expressions.\\n\\nFor example, here are two examples on the `properties` and `source_node` interpretations:\\n\\n```yaml\\n- type: source_node\\n  node_type: User\\n  key:\\n    email: !jmespath email\\n  properties: !jmespath path.to.properties.mapping\\n  normalization:\\n    do_trim_whitespace: true\\n```\\n\\n```yaml\\n- type: properties\\n  properties: !jmespath path.to.properties.mapping\\n  normalization:\\n    do_lowercase_strings: true\\n```\\n\\n**Check out the docs on it [here](/docs/docs/reference/interpreting)**.\\n\\nCore Contributors to this feature include:\\n- [Zach Probst](https://github.com/zprobst)\\n- [Dave Bechberger](https://github.com/bechbd)\\n\\n### Performance Improvements\\n\\nWe\'ve made a small number of performance improvements to the core of nodestream that should result in faster processing times and lower memory usage.\\nMost notably, we\'ve cache the `last_ingested_at` timestamp for nodes and relationships to reduce the number of times we create objects in memory. \\nWe\'ve observed a 10% improvement in processing times and a 5% reduction in memory usage in our testing.\\n\\nCore Contributors to this feature include:\\n- [Zach Probst](https://github.com/zprobst)\\n- [Yason Khaburzaniya](https://github.com/yasonk)\\n- [Grant Hoffman](https://github.com/grantleehoffman)"},{"id":"/2024/04/05/nodestream-sbom-preview","metadata":{"permalink":"/docs/blog/2024/04/05/nodestream-sbom-preview","editUrl":"https://github.com/nodesteram-proj/docs/tree/main/packages/create-docusaurus/templates/shared/blog/2024-04-05-nodestream-sbom-preview/index.md","source":"@site/blog/2024-04-05-nodestream-sbom-preview/index.md","title":"Software Vulnerability Analysis using SBOMs, Amazon Neptune, and Nodestream","description":"Note: Both the Nodestream Neptune and Nodestream SBOM plugins are currently preview releases","date":"2024-04-05T00:00:00.000Z","formattedDate":"April 5, 2024","tags":[{"label":"sbom","permalink":"/docs/blog/tags/sbom"},{"label":"nodestream","permalink":"/docs/blog/tags/nodestream"}],"readingTime":7.835,"hasTruncateMarker":false,"authors":[{"name":"Dave Bechberger","title":"Neptune Plugin Architect and SBOM Plugin Creator","url":"https://github.com/bechbd","imageURL":"https://github.com/bechbd.png","key":"bechbd"}],"frontMatter":{"title":"Software Vulnerability Analysis using SBOMs, Amazon Neptune, and Nodestream","authors":["bechbd"],"tags":["sbom","nodestream"]},"unlisted":false,"prevItem":{"title":"Nodestream 0.12 Release","permalink":"/docs/blog/2024/04/05/nodestream-0-12"},"nextItem":{"title":"Welcome","permalink":"/docs/blog/welcome"}},"content":"**Note**: Both the Nodestream Neptune and Nodestream SBOM plugins are currently preview releases\\n\\nRecently, (March 2024) [a severe vulnerability was found to have been added to a common library, XZ utility](https://www.cisecurity.org/advisory/a-vulnerability-in-xz-utils-could-allow-for-remote-code-execution_2024-033). Unfortunately, serious software vulnerabilities are not isolated incidents, as in late 2021, a [critical security vulnerability was discovered in a commonly used logging library, Log4j](https://www.ncsc.gov.uk/information/log4j-vulnerability-what-everyone-needs-to-know). While the origin of the issues differ, Log4j was an oversight while XZ was an explicit backdoor, the outcome for users was the end same. Once each vulnerability was known, companies and individuals spent many hours combing through countless applications, looking for and patching systems running vulnerable versions of the software.\\n\\nAs this effort was ongoing, many were asking, \\"Isn\'t there a better way to track this information?\\"\\n\\nIn this post, we will discuss the work we have been doing around creating a plugin for Nodestream that provides a unified graph model for SBOMs ingestion and analysis. We will combine this with the plugin for Amazon Neptune to demonstrate how you can find insights for software vulnerabilities in application stacks. Let\u2019s first talk a bit about what an SBOM is and why you should use a graph for analysis.\\n\\n## What is a Software Bill of Materials (SBOM) and why use Graphs\\n\\nA software bill of materials (SBOM) is a critical component of software development and management, helping organizations to improve the transparency, security, and reliability of their software applications. An SBOM acts as an \\"ingredient list\\" of libraries and components of a software application that:\\n\\n- Enables software creators to track dependencies within their applications\\n- Provides security personnel the ability to examine and assess the risk of potential vulnerabilities within an environment\\n- Provides legal personnel with the information needed to assure that a particular software is in compliance with all licensing requirements.\\n\\nA software bill of materials (SBOM) is a comprehensive list of the components, libraries, and dependencies used in a software application or system. It provides a detailed breakdown of the software\'s architecture, including the names, versions, licenses, and optionally the vulnerabilities of each component and describes the complex dependencies and relationships between components of a software system, including multi-level hierarchies and recursive relationships.\\n\\nGraphs are excellent for modeling these kinds of interconnected relationships, with nodes representing components and edges representing dependencies and relationships between these components. Graph data structures handle recursive relationships very naturally, making it easy to analyze networks and flows. Using graph algorithms and metrics, allows you to analyze and identify critical components and dependencies, single points of failure, security vulnerabilities, license compatibilities, etc. for use cases such as:\\n\\n- Dependency graphs - These show how different components in the software relate to and depend on each other. Graphs make these complex relationships easier to visualize.\\n- Vulnerability Graphs - Graphs make it easy to determine and assign associated risks with different vulnerabilities to prioritize fixing known issues.\\n- Supply chain graphs - SBOMs trace the components and dependencies up the software supply chain. Graphs can illustrate the flow of open-source components from lower-level suppliers up to the final product. This helps identify vulnerabilities or licensing issues in the supply chain.\\n\\n## How to use Graphs for SBOM analysis\\n\\nWhile using graphs to assist with SBOM analysis is not new, it also has not been trivial to get the data loaded in due to differing formats, with the two most popular being [CycloneDX](https://cyclonedx.org/) and [SPDX](https://spdx.dev/). To assist with the data loading and analysis, I recently worked on an [SBOM plugin](https://github.com/nodestream-proj/nodestream-plugin-sbom/tree/main) for [Nodestream](https://nodestream-proj.github.io/docs/docs/intro/) to provide a simple way to load SBOMs into an opinionated graph data model from local files, GitHub, or Amazon Inspector. [Nodestream](https://nodestream-proj.github.io/docs/docs/intro/) is a Python framework for performing graph database ETL. The SBOM plugin extends this framework to provide a\\n\\n### Loading Data into SBOMs into our Graph\\n\\nTo get started loading your SBOM files into Amazon Neptune, we first need to setup an Amazon Neptune Analytics Graph as well as a Neptune Notebook to perform our analysis. To configure a Neptune Analytics Graph you can follow the documentation here: https://docs.aws.amazon.com/neptune-analytics/latest/userguide/create-graph-using-console.html\\n\\nNeptune Notebooks is a managed open-source graph-notebook project provides a plethora of Jupyter extensions and sample notebooks that make it easy to interact with and learn to use a Neptune Analytics graph. This can be configured using the documentation here: https://docs.aws.amazon.com/neptune-analytics/latest/userguide/notebooks.html\\n\\nNow that we have setup our database and analysis environment we next need to install the Nodestream plugins for Neptune and SBOM.\\n\\n`pip install -q pyyaml nodestream-plugin-neptune nodestream-plugin-sbom`\\n\\nWith those data files installed, all we need to do is set our configuration in the `nodestream.yaml` file as shown below. In this example, we are going to load the SBOM files for Nodestream, the Nodestream Neptune Plugin, and the Nodestream SBOM plugin into our database, directly from GitHub.\\n\\n```\\nplugins:\\n- name: sbom\\n  config:\\n    repos:[nodestream-proj/nodestream, nodestream-proj/nodestream-plugin-sbom, nodestream-proj/nodestream-plugin-neptune]\\ntargets:\\n  my-neptune:\\n    database: neptune\\n    graph_id: g-<GRAPH ID>\\n    mode: analytics\\n```\\n\\nWith our configuration setup, we can run the import using the following command:\\n\\n`nodestream run sbom_github --target my-neptune`\\n\\nAfter we run the data load, we get a graph that similar to the image below.\\n\\n![SBOM Model Overview](sbom_overview.png \\"SBOM Data Overview\\")\\n\\n### What does our graph look like?\\n\\nLet\u2019s take a look at the types of data that we are storing in our graph. The plugin uses the opinionated graph data model shown below to represent SBOM data files.\\n![SBOM Graph schema](schema.png \\"SBOM Graph Schema\\")\\nThis model contains the following elements:\\n\\n**Node Types**\\n\\n- `Document` - This represents the SBOM document as well as the metadata associated with that SBOM.\\n- `Component` - This represents a specific component of a software system.\\n- `Reference` - This represents a reference to any external system which the system wanted to include as a reference. This can range from package managers, URLs to external websites, etc.\\n- `Vulnerability` - This represents a specific known vulnerability for a component.\\n- `License` - The license for the component or package.\\n\\n**Edge Types**\\n\\n- `DESCRIBES`/`DEPENDS_ON`/`DEPENDENCY_OF`/`DESCRIBED_BY`/`CONTAINS` - This represents the type of relationship between a `Document` and a `Component` in the system.\\n- `REFERS_TO` - This represents a reference between a `Component` and a `Reference`\\n- `AFFECTS` - This represents that a particular `Component` is affected by the connected `Vulnerability`\\n\\nThe properties associated with each element will vary depending on the input format used, and the optional information contained in each file.\\n\\n## Analyzing SBOMs\\n\\nNow that we have our data loaded into our graph, the next step is to start to extract insights into what is actually important in our SBOM data.\\n\\nOne common use case is to investigate shared dependencies across projects. Shared dependencies allow development and security teams to better understand the security posture of the organization through identification of shared risks. Let\'s start by taking a look at the most shared dependencies between these projects using the query below.\\n\\n```\\nMATCH (n:Component)\\nWHERE exists(n.name)\\nCALL neptune.algo.degree(n, {traversalDirection: \'inbound\', edgeLabels: [\'DEPENDS_ON\']})\\nYIELD node, degree\\nRETURN node.name, degree\\nORDER BY degree DESC\\nLIMIT 10\\n```\\n\\nRunning this query will show us that there are quite a few dependencies that are shared across all three projects. To do this analysis, we used a graph algorithm known as [Degree Centrality](https://docs.aws.amazon.com/neptune-analytics/latest/userguide/degree.html) which counts the number of edges connected to a node. This measure of how connected the node is can in turn indicate the node\'s importance and level of influence in the network.\\n![Results](analyze_query_1.png \\"Results\\")\\nRunning the query below shows us that there are 31 `Components` that are shared across all the projects.\\n\\n```\\nMATCH (n:Component)\\nWHERE exists(n.name)\\nCALL neptune.algo.degree(n, {traversalDirection: \'inbound\', edgeLabels: [\'DEPENDS_ON\']})\\nYIELD node, degree\\nWHERE degree=3\\nRETURN count(node)\\n```\\n\\nGiven that this is a closely connected group of projects, it is not a surprise that there are many shared components. Given that one of the strengths of graphs is the ability to visualize the connectedness between data, let\u2019s take a look at how they are connected.\\n\\n```\\nMATCH (n:Component)\\nWHERE exists(n.name)\\nCALL neptune.algo.degree(n, {traversalDirection: \'inbound\', edgeLabels: [\'DEPENDS_ON\']})\\nYIELD node, degree\\nWHERE degree = 3\\nWITH node, degree\\nMATCH p=(node)-[]-()\\nRETURN p\\n```\\n\\n![Results](analyze_query_2.png \\"Results\\")\\n\\nAnother common use case is to investigate licensing across multiple projects. This sort of investigation benefits from the connectedness across the graph by leveraging the connectedness to find how component licenses are connected to each other. Let\u2019s take a look at what other licenses are associated with the `lgpl-2.1-or-later` licensed components.\\n\\n```\\nMATCH p=(l:License)<-[:LICENSED_BY]-(:Component)<-[:DEPENDS_ON]-(:Document)\\n-[:DEPENDS_ON]->(:Component)-[:LICENSED_BY]->(l2)\\nWHERE l.name = \'lgpl-2.1-or-later\' and l<>l2\\nRETURN DISTINCT l2.name\\n```\\n\\n![Results](analyze_query_3.png \\"Results\\")\\n\\nAs we see, there are quite a few other licenses used in these projects. We can leverage the visual nature of graph results to gain some insight into how components are connected. In this case, let\u2019s see how components with the `lgpl-2.1-or-later` are connected to components with the `unlicense`.\\n\\n```\\nMATCH p=(l:License)\u2190[:LICENSED_BY]-(:Component)\u2190[:DEPENDS_ON]-(:Document)\\n-[:DEPENDS_ON]\u2192(:Component)-[:LICENSED_BY]\u2192(l2)\\nWHERE l.name = \'lgpl-2.1-or-later\' and l<>l2\\nRETURN DISTINCT l2.name\\n```\\n\\n![Results](analyze_query_4.png \\"Results\\")\\n\\nWe see that there exists one path in our graph between these two licenses.\\n\\n## Next Steps\\n\\nAs we have seen, using graphs to perform analysis of SBOM data can be a powerful tool in your toolbox to gain insights into the connections between software projects. What I have shown here is only the beginning of the types of analysis you can perform with this data. For a more detailed walkthrough of using graphs for SBOM analysis, I recommend taking a look at the following notebooks:\\n\\n* [SBOM Dependency Analysis](https://github.com/aws/graph-notebook/blob/main/src/graph_notebook/notebooks/02-Neptune-Analytics/03-Sample-Use-Cases/03-Software-Bill-Of-Materials/01-SBOM-Dependency-Analysis.ipynb)\\n\\n* [SBOM Vulnerability Analysis](https://github.com/aws/graph-notebook/blob/main/src/graph_notebook/notebooks/02-Neptune-Analytics/03-Sample-Use-Cases/03-Software-Bill-Of-Materials/02-SBOM-Vulnerability-Analysis.ipynb)"},{"id":"welcome","metadata":{"permalink":"/docs/blog/welcome","editUrl":"https://github.com/nodesteram-proj/docs/tree/main/packages/create-docusaurus/templates/shared/blog/2024-03-30-welcome.md","source":"@site/blog/2024-03-30-welcome.md","title":"Welcome","description":"Welcome to the new nodestream documentation and project site!","date":"2024-03-30T00:00:00.000Z","formattedDate":"March 30, 2024","tags":[{"label":"welcome","permalink":"/docs/blog/tags/welcome"},{"label":"nodestream","permalink":"/docs/blog/tags/nodestream"}],"readingTime":0.545,"hasTruncateMarker":false,"authors":[{"name":"Zach Probst","title":"Maintainer of Nodestream","url":"https://github.com/zprobst","imageURL":"https://github.com/zprobst.png","key":"zprobst"}],"frontMatter":{"slug":"welcome","title":"Welcome","authors":["zprobst"],"tags":["welcome","nodestream"]},"unlisted":false,"prevItem":{"title":"Software Vulnerability Analysis using SBOMs, Amazon Neptune, and Nodestream","permalink":"/docs/blog/2024/04/05/nodestream-sbom-preview"}},"content":"Welcome to the new nodestream documentation and project site! \\nWe are excited to share with you the new features and improvements we have been working on.\\n\\nWe have been working hard to improve the documentation and make it easier to use and navigate.\\nWe have also been working on improving the project site to make it easier to find the information you need.\\n\\nWe hope you find the new documentation and project site helpful and easy to use!\\n\\nBy the way, thanks to the [Docusaurus](https://docusaurus.io/) team for creating such a great tool!\\n\\nIf you have any questions or feedback, please feel free to reach out to us on [GitHub](https://github.com/nodestream-proj/nodestream)!"}]}')}}]);